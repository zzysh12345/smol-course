# Preference Alignment (Alinhamento de PreferÃªncia)

Este mÃ³dulo abrange tÃ©cnicas para alinhar modelos de linguagem com preferÃªncias humanas. Enquanto o ajuste fino supervisionado ajuda os modelos a aprender tarefas, o alinhamento de preferÃªncia incentiva os resultados a corresponder Ã s expectativas e valores humanos.

## VisÃ£o Geral

MÃ©todos de alinhamento tÃ­picos envolvem vÃ¡rios estÃ¡gios:
1. Ajuste fino supervisionado (SFT) para adaptar os modelos a domÃ­nios especÃ­ficos
2. Alinhamento de preferÃªncia (como RLHF ou DPO) para melhorar a qualidade da resposta

Abordagens alternativas como ORPO combinam ajuste de instruÃ§Ã£o e alinhamento de preferÃªncia em um Ãºnico processo. Aqui, vamos focar nos algoritmos DPO e ORPO.

Se vocÃª quer aprender mais sobre as diferentes tÃ©cnicas de alinhamento, vocÃª pode ler mais sobre isso no [Argilla Blog](https://argilla.io/blog/mantisnlp-rlhf-part-8). 

### 1ï¸âƒ£ OtimizaÃ§Ã£o de PreferÃªncia Direta (DPO - Direct Preference Optimization)

OtimizaÃ§Ã£o de preferÃªncia direta (DPO) simplifica o alinhamento de preferÃªncia otimizando diretamente os modelos usando dados de preferÃªncia. Essa abordagem elimina a necessidade de usar modelos de recompensa que nÃ£o fazem parte do sistema e aprendizado de reforÃ§o complexo, tornando-o mais estÃ¡vel e eficiente do que o tradicional aprendizado de reforÃ§o com o feedback humano (RLHF). Para mais detalhes,vocÃª pode ler mais em [DocumentaÃ§Ã£o sobre otimizaÃ§Ã£o de preferÃªncia direta (DPO)](./dpo.md).


### 2ï¸âƒ£ OtimizaÃ§Ã£o de PreferÃªncias de RazÃ£o de Chances (ORPO - Odds Ratio Preference Optimization)

ORPO introduz uma abordagem combinada para ajuste de instruÃ§Ã£o e alinhamento de preferÃªncia em um Ãºnico processo. Ele modifica o objetivo padrÃ£o de modelagem de linguagem combinando a perda de log-verossimilhanÃ§a negativa com um termo de razÃ£o de chances em um nÃ­vel de token. A abordagem apresenta um processo de treinamento unificado de estÃ¡gio Ãºnico, arquitetura sem modelo de referÃªncia e eficiÃªncia computacional aprimorada. O ORPO apresentou resultados impressionantes em vÃ¡rios benchmarks, demonstrando melhor desempenho no AlpacaEval em comparaÃ§Ã£o com os mÃ©todos tradicionais. Para obter mais detalhes, consulte a [DocumentaÃ§Ã£o sobre OtimizaÃ§Ã£o de PreferÃªncias de RazÃ£o de Chances (ORPO)](./orpo.md)

## Caderno de ExercÃ­cios

| TÃ­tulo | DescriÃ§Ã£o | ExercÃ­cio | Link | Colab |
|-------|-------------|----------|------|-------|
| Treinamento em DPO | Aprenda a treinar modelos usando a OtimizaÃ§Ã£o Direta de PreferÃªncia | ğŸ¢ Treine um modelo usando o conjunto de dados Anthropic HH-RLHF<br>ğŸ• Use seu prÃ³prio conjunto de dados de preferÃªncias<br>ğŸ¦ Experimente diferentes conjuntos de dados de preferÃªncias e tamanhos de modelos | [ExercÃ­cio](./notebooks/dpo_finetuning_example.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/2_preference_alignment/notebooks/dpo_finetuning_example.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
| Treinamento em ORPO | Aprenda a treinar modelos usando a otimizaÃ§Ã£o de preferÃªncias de razÃ£o de chances | ğŸ¢ Treine um modelo usando instruÃ§Ãµes e dados de preferÃªncias<br>ğŸ• Experimente com diferentes pesos de perda<br>ğŸ¦ Comparar os resultados de ORPO com DPO | [ExercÃ­cio](./notebooks/orpo_finetuning_example.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/2_preference_alignment/notebooks/orpo_finetuning_example.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |


## ReferÃªncias

- [DocumentaÃ§Ã£o do TRL](https://huggingface.co/docs/trl/index) - DocumentaÃ§Ã£o do mÃ³dulo Transformers Reinforcement Learning (TRL), que implementa vÃ¡rias tÃ©cnicas de alinhamento, inclusive a DPO.
- [Artigo sobre DPO](https://arxiv.org/abs/2305.18290) - Documento de pesquisa original que apresenta a OtimizaÃ§Ã£o Direta de PreferÃªncia como uma alternativa mais simples ao RLHF que otimiza diretamente os modelos de linguagem usando dados de preferÃªncia.
- [Artigo sobre ORPO](https://arxiv.org/abs/2403.07691) - Apresenta a OtimizaÃ§Ã£o de preferÃªncias de razÃ£o de chances, uma nova abordagem que combina o ajuste de instruÃ§Ãµes e o alinhamento de preferÃªncias em um Ãºnico estÃ¡gio de treinamento.
- [Guia Argilla RLHF](https://argilla.io/blog/mantisnlp-rlhf-part-8/) - Um guia que explica diferentes tÃ©cnicas de alinhamento, incluindo RLHF, DPO e suas implementaÃ§Ãµes prÃ¡ticas.
- [Postagem de blog sobre DPO](https://huggingface.co/blog/dpo-trl) - Guia prÃ¡tico sobre a implementaÃ§Ã£o de DPO usando a biblioteca TRL com exemplos de cÃ³digo e prÃ¡ticas recomendadas.
- [Exemplo de script TRL no DPO](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py) - Script de exemplo completo que demonstra como implementar o treinamento em DPO usando a biblioteca TRL.
- [Exemplo de script TRL no ORPO](https://github.com/huggingface/trl/blob/main/examples/scripts/orpo.py) - ImplementaÃ§Ã£o de referÃªncia do treinamento ORPO usando a biblioteca TRL com opÃ§Ãµes de configuraÃ§Ã£o detalhadas.
- [Manual de alinhamento do Hugging Face](https://github.com/huggingface/alignment-handbook) - Guias de recursos e base de cÃ³digo para alinhamento de modelos de linguagem usando vÃ¡rias tÃ©cnicas, incluindo SFT, DPO e RLHF.