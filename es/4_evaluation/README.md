# Evaluaci贸n

La evaluaci贸n es un paso cr铆tico en el desarrollo y despliegue de modelos de lenguaje. Nos permite entender qu茅 tan bien funcionan nuestros modelos en diferentes capacidades e identificar 谩reas de mejora. Este m贸dulo cubre tanto los "becnhmarks" est谩ndares como los enfoques de evaluaci贸n espec铆ficos para evaluar de manera integral tu modelo **smol**.

Usaremos [`lighteval`](https://github.com/huggingface/lighteval), una poderosa biblioteca de evaluaci贸n desarrollada por Hugging Face que se integra perfectamente con el ecosistema de Hugging Face. Para una explicaci贸n m谩s detallada sobre los conceptos y mejores pr谩cticas de evaluaci贸n, consulta la [gu铆a de evaluaci贸n](https://github.com/huggingface/evaluation-guidebook).

## Descripci贸n del M贸dulo

Una estrategia de evaluaci贸n completa examina m煤ltiples aspectos del rendimiento del modelo. Evaluamos capacidades espec铆ficas en tareas como responder preguntas y resumir textos, para entender c贸mo el modelo maneja diferentes tipos de problemas. Medimos la calidad del "output" mediante factores como coherencia y precisi贸n. A su vez, la evaluaci贸n de seguridad ayuda a identificar posibles "outputs" da帽inas o sesgos. Finalmente, las pruebas de experticia en 谩reas especf铆cios verifican el conocimiento especializado del modelo en tu campo objetivo.

## Contenidos

### 1锔 [Evaluaciones Autom谩ticas](./automatic_benchmarks.md)
Aprende a evaluar tu modelo utilizando "benchmarks" y m茅tricas estandarizadas. Exploraremos "benchmarks" comunes como MMLU y TruthfulQA, entenderemos las m茅tricas clave de evaluaci贸n y configuraciones, y cubriremos mejores pr谩cticas para una evaluaci贸n reproducible.

### 2锔 [Evaluaci贸n Personalizada en un Dominio](./custom_evaluation.md)
Descubre c贸mo crear flujos de evaluaci贸n adaptados a tus casos de uso espec铆ficos. Te guiaremos en el dise帽o de tareas de evaluaci贸n personalizadas, la implementaci贸n de m茅tricas especializadas y la construcci贸n de conjuntos de datos de evaluaci贸n que se ajusten a tus necesidades.

### 3锔 [Proyecto de Evaluaci贸n en un Dominio](./project/README.md)
Sigue un ejemplo completo de c贸mo construir un flujo de evaluaci贸n espec铆fico para un dominio. Aprender谩s a generar conjuntos de datos de evaluaci贸n, usar Argilla para la anotaci贸n de datos, crear conjuntos de datos estandarizados y evaluar modelos utilizando LightEval.

### Cuadernos de Ejercicios

| T铆tulo | Descripci贸n | Ejercicio | Enlace | Colab |
|--------|-------------|----------|-------|-------|
| Eval煤a y analiza tu LLM | Aprende a usar LightEval para evaluar y comparar modelos en dominios espec铆ficos |  Usa tareas del dominio m茅dico para evaluar un modelo <br>  Crea una evaluaci贸n de dominio con diferentes tareas MMLU <br>  Dise帽a una tarea de evaluaci贸n personalizada para tu dominio | [Cuaderno](./notebooks/lighteval_evaluate_and_analyse_your_LLM.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/4_evaluation/notebooks/lighteval_evaluate_and_analyse_your_LLM.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |

## Recursos

- [Gu铆a de Evaluaci贸n](https://github.com/huggingface/evaluation-guidebook) - Gu铆a completa para la evaluaci贸n de modelos de lenguaje
- [Documentaci贸n de LightEval](https://github.com/huggingface/lighteval) - Documentaci贸n oficial de la biblioteca LightEval
- [Documentaci贸n de Argilla](https://docs.argilla.io) - Aprende sobre la plataforma de anotaci贸n Argilla
- [Paper de MMLU](https://arxiv.org/abs/2009.03300) - Art铆culo sobre el benchmark MMLU
- [Crear una Tarea Personalizada](https://github.com/huggingface/lighteval/wiki/Adding-a-Custom-Task)
- [Crear una M茅trica Personalizada](https://github.com/huggingface/lighteval/wiki/Adding-a-New-Metric)
- [Usar m茅tricas existentes](https://github.com/huggingface/lighteval/wiki/Metric-List)
