{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huggingface/smol-course/blob/main/1_instruction_tuning/notebooks/chat_templates_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZAvFVIAtFlq"
      },
      "source": [
        "# Exploring Chat Templates with SmolLM2 and Llama 3.2\n",
        "\n",
        "This notebook demonstrates how to use chat templates with the `SmolLM2` and `Llama 3.2` models. Chat templates help structure interactions between users and AI models, ensuring consistent and contextually appropriate responses."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries for Google Colab\n",
        "!pip install trl -q"
      ],
      "metadata": {
        "id": "K-lZu8JvtwUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnHzBR7vtFlr"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import setup_chat_format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTVOqbuetFlr"
      },
      "source": [
        "## SmolLM2 Chat Template\n",
        "\n",
        "Let's explore how to use a chat template with the `SmolLM2` model. We'll define a simple conversation and apply the chat template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrxh0oX6tFls"
      },
      "outputs": [],
      "source": [
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
        "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkJwILrbtFls"
      },
      "outputs": [],
      "source": [
        "# Define messages for SmolLM2\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"I'm doing well, thank you! How can I assist you today?\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4dgtjstFls"
      },
      "source": [
        "# Apply chat template without tokenization\n",
        "\n",
        "The tokenizer represents the conversation as a string with special tokens to describe the role of the user and the assistant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbAg-5x-tFls"
      },
      "outputs": [],
      "source": [
        "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "print(\"Conversation with template:\", input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfvdglOqtFls"
      },
      "source": [
        "# Decode the conversation\n",
        "\n",
        "Note that the conversation is represented as above but with a further assistant message.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXUVdPeytFls"
      },
      "outputs": [],
      "source": [
        "input_text = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
        "\n",
        "print(\"Conversation decoded:\", tokenizer.decode(token_ids=input_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcZQpspEtFlt"
      },
      "source": [
        "# Tokenize the conversation\n",
        "\n",
        "Of course, the tokenizer also tokenizes the conversation and special token as ids that relate to the model's vocabulary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc2PLxAMtFlt"
      },
      "outputs": [],
      "source": [
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "print(\"Conversation tokenized:\", input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3eNp9a0tFlt"
      },
      "source": [
        "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px'>\n",
        "    <h2 style='margin: 0;color:blue'>Exercise: Process a dataset for SFT</h2>\n",
        "    <p>Take a dataset from the Hugging Face hub and process it for SFT. </p>\n",
        "    <p><b>Difficulty Levels</b></p>\n",
        "    <p>üê¢ Convert the `HuggingFaceTB/smoltalk` dataset into chatml format.</p>\n",
        "    <p>üêï Convert the `openai/gsm8k` dataset into chatml format.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbkXV2_ItFlt"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "display(\n",
        "    HTML(\"\"\"<iframe\n",
        "  src=\"https://huggingface.co/datasets/HuggingFaceTB/smoltalk/embed/viewer/all/train?row=0\"\n",
        "  frameborder=\"0\"\n",
        "  width=\"100%\"\n",
        "  height=\"360px\"\n",
        "></iframe>\n",
        "\"\"\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p3atw4_tFlu"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"HuggingFaceTB/smoltalk\", \"everyday-conversations\")\n",
        "\n",
        "\n",
        "def process_dataset(sample):\n",
        "    # TODO: üê¢ Convert the sample into a chat format\n",
        "    return sample\n",
        "\n",
        "\n",
        "ds = ds.map(process_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81fQeazltFlu"
      },
      "outputs": [],
      "source": [
        "display(\n",
        "    HTML(\"\"\"<iframe\n",
        "  src=\"https://huggingface.co/datasets/openai/gsm8k/embed/viewer/main/train\"\n",
        "  frameborder=\"0\"\n",
        "  width=\"100%\"\n",
        "  height=\"360px\"\n",
        "></iframe>\n",
        "\"\"\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bWUSv7NMtFlu"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
        "\n",
        "\n",
        "def process_dataset(sample):\n",
        "    # TODO: üêï Convert the sample into a chat format\n",
        "    return sample\n",
        "\n",
        "\n",
        "ds = ds.map(process_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlXCuRKotFlu"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated how to apply chat templates to different models, `SmolLM2`. By structuring interactions with chat templates, we can ensure that AI models provide consistent and contextually relevant responses.\n",
        "\n",
        "In the exercise you tried out converting a dataset into chatml format. Luckily, TRL will do this for you, but it's useful to understand what's going on under the hood."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}