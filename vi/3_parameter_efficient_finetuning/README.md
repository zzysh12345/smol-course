# Tinh chá»‰nh hiá»‡u quáº£ tham sá»‘ (Parameter-Efficient Fine-Tuning - PEFT)

Khi cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ ngÃ y cÃ ng lá»›n hÆ¡n, viá»‡c tinh chá»‰nh truyá»n thá»‘ng trá»Ÿ nÃªn ngÃ y cÃ ng thÃ¡ch thá»©c. Viá»‡c tinh chá»‰nh Ä‘áº§y Ä‘á»§ má»™t mÃ´ hÃ¬nh vá»›i 1.7B tham sá»‘ Ä‘Ã²i há»i bá»™ nhá»› GPU lá»›n, viá»‡c lÆ°u trá»¯ cÃ¡c báº£n sao mÃ´ hÃ¬nh riÃªng biá»‡t tá»‘n kÃ©m, vÃ  cÃ³ nguy cÆ¡ lÃ m máº¥t Ä‘i cÃ¡c kháº£ nÄƒng ban Ä‘áº§u cá»§a mÃ´ hÃ¬nh. CÃ¡c phÆ°Æ¡ng phÃ¡p tinh chá»‰nh hiá»‡u quáº£ tham sá»‘ (PEFT) giáº£i quyáº¿t nhá»¯ng thÃ¡ch thá»©c nÃ y báº±ng cÃ¡ch chá»‰ Ä‘iá»u chá»‰nh má»™t táº­p nhá» cÃ¡c tham sá»‘ mÃ´ hÃ¬nh trong khi giá»¯ nguyÃªn pháº§n lá»›n mÃ´ hÃ¬nh.

Tinh chá»‰nh truyá»n thá»‘ng cáº­p nháº­t táº¥t cáº£ cÃ¡c tham sá»‘ mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, Ä‘iá»u nÃ y trá»Ÿ nÃªn khÃ´ng kháº£ thi vá»›i cÃ¡c mÃ´ hÃ¬nh lá»›n. CÃ¡c phÆ°Æ¡ng phÃ¡p PEFT giá»›i thiá»‡u cÃ¡ch tiáº¿p cáº­n Ä‘á»ƒ Ä‘iá»u chá»‰nh mÃ´ hÃ¬nh sá»­ dá»¥ng Ã­t tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n hÆ¡n - thÆ°á»ng Ã­t hÆ¡n 1% kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh gá»‘c. Viá»‡c giáº£m Ä‘Ã¡ng ká»ƒ sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n cho phÃ©p:

- Tinh chá»‰nh trÃªn pháº§n cá»©ng tiÃªu dÃ¹ng vá»›i bá»™ nhá»› GPU háº¡n cháº¿ 
- LÆ°u trá»¯ nhiá»u phiÃªn báº£n Ä‘iá»u chá»‰nh (adapters) cho tá»«ng tÃ¡c vá»¥ má»™t cÃ¡ch hiá»‡u quáº£
- Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n trong cÃ¡c trÆ°á»ng há»£p dá»¯ liá»‡u Ã­t
- Chu ká»³ huáº¥n luyá»‡n vÃ  thá»­ nghiá»‡m nhanh hÆ¡n

## CÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³

Trong chÆ°Æ¡ng nÃ y, chÃºng ta sáº½ tÃ¬m hiá»ƒu hai phÆ°Æ¡ng phÃ¡p PEFT phá»• biáº¿n:

### 1ï¸âƒ£ PhÆ°Æ¡ng PhÃ¡p LoRA (Low-Rank Adaptation)

LoRA Ä‘Ã£ ná»•i lÃªn nhÆ° phÆ°Æ¡ng phÃ¡p PEFT Ä‘Æ°á»£c Ã¡p dá»¥ng rá»™ng rÃ£i nháº¥t, cung cáº¥p giáº£i phÃ¡p hoÃ n háº£o cho viá»‡c Ä‘iá»u chá»‰nh mÃ´ hÃ¬nh hiá»‡u quáº£ mÃ  khÃ´ng tá»‘n nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n. Thay vÃ¬ sá»­a Ä‘á»•i toÃ n bá»™ mÃ´ hÃ¬nh, **LoRA Ä‘Æ°a cÃ¡c ma tráº­n cÃ³ thá»ƒ huáº¥n luyá»‡n vÃ o cÃ¡c lá»›p attention cá»§a mÃ´ hÃ¬nh.** CÃ¡ch tiáº¿p cáº­n nÃ y thÆ°á»ng giáº£m cÃ¡c tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n khoáº£ng 90% trong khi váº«n duy trÃ¬ hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§. ChÃºng ta sáº½ khÃ¡m phÃ¡ LoRA trong pháº§n [LoRA (Low-Rank Adaptation)](./lora_adapters.md).

### 2ï¸âƒ£ PhÆ°Æ¡ng PhÃ¡p Äiá»u Chá»‰nh Chá»‰ Thá»‹ (Prompt Tuning)

Prompt tuning cung cáº¥p cÃ¡ch tiáº¿p cáº­n **tháº­m chÃ­ nháº¹ hÆ¡n** báº±ng cÃ¡ch **thÃªm cÃ¡c token cÃ³ thá»ƒ huáº¥n luyá»‡n vÃ o Ä‘áº§u vÃ o** thay vÃ¬ sá»­a Ä‘á»•i trá»ng sá»‘ mÃ´ hÃ¬nh. Prompt tuning Ã­t phá»• biáº¿n hÆ¡n LoRA, nhÆ°ng cÃ³ thá»ƒ lÃ  ká»¹ thuáº­t há»¯u Ã­ch Ä‘á»ƒ nhanh chÃ³ng Ä‘iá»u chá»‰nh mÃ´ hÃ¬nh cho cÃ¡c tÃ¡c vá»¥ hoáº·c lÄ©nh vá»±c má»›i. ChÃºng ta sáº½ khÃ¡m phÃ¡ prompt tuning trong pháº§n [Prompt Tuning](./prompt_tuning.md).

## Notebooks bÃ i táº­p

| TiÃªu Ä‘á» | MÃ´ táº£ | BÃ i táº­p | Link | Colab |
|---------|--------|---------|------|-------|
| Tinh chá»‰nh LoRA | Há»c cÃ¡ch tinh chá»‰nh mÃ´ hÃ¬nh sá»­ dá»¥ng LoRA adapters | ğŸ¢ Huáº¥n luyá»‡n mÃ´ hÃ¬nh sá»­ dá»¥ng LoRA<br>ğŸ• Thá»­ nghiá»‡m vá»›i cÃ¡c giÃ¡ trá»‹ rank khÃ¡c nhau<br>ğŸ¦ So sÃ¡nh hiá»‡u suáº¥t vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§ | [Notebook](./notebooks/finetune_sft_peft.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/3_parameter_efficient_finetuning/notebooks/finetune_sft_peft.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
| Táº£i LoRA Adapters | Há»c cÃ¡ch táº£i vÃ  sá»­ dá»¥ng LoRA adapters Ä‘Ã£ huáº¥n luyá»‡n | ğŸ¢ Táº£i adapters Ä‘Ã£ huáº¥n luyá»‡n trÆ°á»›c<br>ğŸ• Gá»™p adapters vá»›i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ<br>ğŸ¦ Chuyá»ƒn Ä‘á»•i giá»¯a nhiá»u adapters | [Notebook](./notebooks/load_lora_adapter.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/3_parameter_efficient_finetuning/notebooks/load_lora_adapter.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |

## TÃ i liá»‡u tham kháº£o
- [TÃ i liá»‡u PEFT](https://huggingface.co/docs/peft)
- [BÃ i bÃ¡o nghiÃªn cá»©u LoRA](https://arxiv.org/abs/2106.09685)
- [BÃ i bÃ¡o nghiÃªn cá»©u QLoRA](https://arxiv.org/abs/2305.14314)
- [BÃ i bÃ¡o nghiÃªn cá»©u Prompt Tuning](https://arxiv.org/abs/2104.08691)
- [HÆ°á»›ng dáº«n sá»­ dá»¥ng PEFT cá»§a Hugging Face](https://huggingface.co/blog/peft)
- [CÃ¡ch Tinh chá»‰nh LLM vá»›i Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl)
- [ThÆ° viá»‡n TRL](https://huggingface.co/docs/trl/index)