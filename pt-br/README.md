![smolcourse image](../banner.png)

# a smol course (um curso miudinho)

Este Ã© um curso prÃ¡tico sobre alinhar modelos de linguagem para o seu caso de uso especÃ­fico. Ã‰ uma maneira Ãºtil de comeÃ§ar a alinhar modelos de linguagem, porque tudo funciona na maioria das mÃ¡quinas locais. Existem requisitos mÃ­nimos de GPU e nenhum serviÃ§o pago. O curso Ã© baseado na sÃ©rie de modelos de [SmolLM2](https://github.com/huggingface/smollm/tree/main), mas vocÃª pode transferir as habilidades que aprende aqui para modelos maiores ou outros pequenos modelos de linguagem.

<a href="http://hf.co/join/discord">
<img src="https://img.shields.io/badge/Discord-7289DA?&logo=discord&logoColor=white"/>
</a>

<div style="background: linear-gradient(to right, #e0f7fa, #e1bee7, orange); padding: 20px; border-radius: 5px; margin-bottom: 20px; color: purple;">
    <h2>A participaÃ§Ã£o Ã© aberta a todos, gratuita e jÃ¡ estÃ¡ disponÃ­vel!</h2>
    <p>Este curso Ã© aberto e avaliado por pares (peer reviewed). Para comeÃ§ar o curso, <strong>abra um pull request (PR)</strong> e envie seu trabalho para revisÃ£o. Aqui estÃ£o as etapas:</p>
    <ol>
        <li>DÃª um fork no repositÃ³rio <a href="https://github.com/huggingface/smol-course/fork">aqui</a></li>
        <li>Leia o material, faÃ§a alteraÃ§Ãµes, faÃ§a os exercÃ­cios, adicione seus prÃ³prios exemplos</li>
        <li>Abra um PR no branch december_2024</li>
        <li>Tenha seu material revisado e mesclado no branch principal</li>
    </ol>
    <p>Isso deve te ajudar a aprender e a construir um curso feito pela comunidade, que estÃ¡ sempre melhorando.</p>
</div>

Podemos discutir o processo neste [tÃ³pico de discussÃ£o](https://github.com/huggingface/smol-course/discussions/2#discussion-7602932).

## SumÃ¡rio do Curso

Este curso fornece uma abordagem prÃ¡tica para trabalhar com pequenos modelos de linguagem, desde o treinamento inicial atÃ© a implantaÃ§Ã£o de produÃ§Ã£o.

| MÃ³dulo | DescriÃ§Ã£o | Status | Data de LanÃ§amento |
|--------|-------------|---------|--------------|
| [Instruction Tuning (Ajuste de InstruÃ§Ã£o)](./1_instruction_tuning) | Aprenda sobre o ajuste fino supervisionado, modelos de bate-papo e a fazer o modelo seguir instruÃ§Ãµes bÃ¡sicas | âœ… Completo | 3 Dez, 2024 |
| [Preference Alignment (Alinhamento de PreferÃªncia)](./2_preference_alignment) | Explore tÃ©cnicas DPO e ORPO para alinhar modelos com preferÃªncias humanas | âœ… Completo  | 6 Dez, 2024 |
| [Parameter-efficient Fine-tuning (Ajuste Fino com EficiÃªncia de ParÃ¢metro)](./3_parameter_efficient_finetuning) | Aprenda sobre LoRA, ajuste de prompt e mÃ©todos de adaptaÃ§Ã£o eficientes | âœ… Completo | 9 Dez, 2024 |
| [Evaluation (AvaliaÃ§Ã£o)](./4_evaluation) | Use benchmarks automÃ¡ticos e crie avaliaÃ§Ãµes de domÃ­nio personalizadas | âœ… Completo | 13 Dez, 2024 |
| [Vision-language Models (Modelos de Conjunto VisÃ£o-linguagem)](./5_vision_language_models) | Adapte modelos multimodais para tarefas visÃ£o-linguagem | âœ… Completo | 16 Dez, 2024 |
| [Synthetic Datasets (Conjuntos de Dados SintÃ©ticos)](./6_synthetic_datasets) | Criar e validar conjuntos de dados sintÃ©ticos para treinamento | [ğŸš§ Em Progresso](https://github.com/huggingface/smol-course/issues/83) | 20 Dez, 2024 |
| [Inference (InferÃªncia)](./7_inference) | Infira modelos com eficiÃªncia | ğŸ“ Planejado | 23 Dez, 2024 |
| Projeto Experimental | Use o que vocÃª aprendeu para ser o top 1 na tabela de classificaÃ§Ã£o! | [ğŸš§ Em Progresso](https://github.com/huggingface/smol-course/pull/97) | Dec 23, 2024 |

## Por Que Pequenos Modelos de Linguagem?

Embora os grandes modelos de linguagem tenham mostrado recursos e capacidades impressionantes, eles geralmente exigem recursos computacionais significativos e podem ser exagerados para aplicativos focados. Os pequenos modelos de linguagem oferecem vÃ¡rias vantagens para aplicativos de domÃ­nios especÃ­ficos:

- **EficiÃªncia**: Requer significativamente menos recursos computacionais para treinar e implantar
- **PersonalizaÃ§Ã£o**: Mais fÃ¡cil de ajustar e se adaptar a domÃ­nios especÃ­ficos
- **Controle**: Melhor compreensÃ£o e controle do comportamento do modelo
- **Custo**: Menores custos operacionais para treinamento e inferÃªncia
- **Privacidade**: Pode ser executado localmente sem enviar dados para APIs externas
- **Tecnologia Verde**: Defende o uso eficiente de recursos com reduÃ§Ã£o da pegada de carbono 
- **Desenvolvimento de Pesquisa AcadÃªmica Mais FÃ¡cil**: Oferece um ponto de partida fÃ¡cil para a pesquisa acadÃªmica com LLMs de ponta com menos restriÃ§Ãµes logÃ­sticas

## PrÃ©-requisitos

Antes de comeÃ§ar, verifique se vocÃª tem o seguinte:
- Entendimento bÃ¡sico de machine learning e natural language processing.
- Familiaridade com Python, PyTorch e o mÃ³dulo `transformers`.
- Acesso a um modelo de linguagem prÃ©-treinado e um conjunto de dados rotulado.

## InstalaÃ§Ã£o

Mantemos o curso como um pacote para que vocÃª possa instalar dependÃªncias facilmente por meio de um gerenciador de pacotes. Recomendamos [uv](https://github.com/astral-sh/uv) para esse fim, mas vocÃª pode usar alternativas como `pip` ou` pdm`.

### Usando `uv`

Com o `uv` instalado, vocÃª pode instalar o curso deste modo:

```bash
uv venv --python 3.11.0
uv sync
```

### Usando `pip`

Todos os exemplos sÃ£o executados no mesmo ambiente **python 3.11**, entÃ£o vocÃª deve criar um ambiente e instalar dependÃªncias desta maneira:

```bash
# python -m venv .venv
# source .venv/bin/activate
pip install -r requirements.txt
```

### Google Colab

**A partir do Google Colab** vocÃª precisarÃ¡ instalar dependÃªncias de maneira flexÃ­vel com base no hardware que estÃ¡ usando. Pode fazer deste jeito:

```bash
pip install transformers trl datasets huggingface_hub
```

## Engajamento

Vamos compartilhar isso, desse jeito um monte de gente vai poder aprender a ajustar LLMs sem precisar de um computador super caro.

[![GrÃ¡fico de HistÃ³rico de Estrelas](https://api.star-history.com/svg?repos=huggingface/smol-course&type=Date)](https://star-history.com/#huggingface/smol-course&Date)
