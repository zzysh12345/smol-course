# Alineaci贸n de Preferencias

Este m贸dulo cubre t茅cnicas para alinear los modelos de lenguaje con las preferencias humanas. Mientras que el ajuste supervisado ayuda a los modelos a aprender tareas, la alineaci贸n de preferencias fomenta que las salidas coincidan con las expectativas y valores humanos.

## Visi贸n General

Los m茅todos t铆picos de alineaci贸n involucran varias etapas:
1. Ajuste Supervisado (SFT) para adaptar los modelos a dominios espec铆ficos.
2. Alineaci贸n de preferencias (como RLHF o DPO) para mejorar la calidad de las respuestas.

Enfoques alternativos como ORPO combinan el ajuste de instrucciones y la alineaci贸n de preferencias en un solo proceso. Aqu铆, nos enfocaremos en los algoritmos DPO y ORPO.

Si deseas aprender m谩s sobre las diferentes t茅cnicas de alineaci贸n, puedes leer m谩s sobre ellas en el [Blog de Argilla](https://argilla.io/blog/mantisnlp-rlhf-part-8).

### 1锔 Optimizaci贸n Directa de Preferencias (DPO)

La Optimizaci贸n Directa de Preferencias (DPO) simplifica la alineaci贸n de preferencias optimizando directamente los modelos utilizando datos de preferencias. Este enfoque elimina la necesidad de modelos de recompensa separados y de un complejo aprendizaje por refuerzo, lo que lo hace m谩s estable y eficiente que el aprendizaje por refuerzo de retroalimentaci贸n humana (RLHF) tradicional. Para m谩s detalles, puedes consultar la [documentaci贸n de DPO](./dpo.md).

### 2锔 Optimizaci贸n de Preferencias por Raz贸n de Probabilidades (ORPO)

ORPO introduce un enfoque combinado para el ajuste de instrucciones y la alineaci贸n de preferencias en un solo proceso. Modifica el objetivo est谩ndar de modelado de lenguaje al combinar la p茅rdida de log-verosimilitud negativa con un t茅rmino de raz贸n de probabilidades a nivel de tokens. El enfoque presenta un proceso de entrenamiento unificado de una sola etapa, una arquitectura sin referencia al modelo y una mayor eficiencia computacional. ORPO ha mostrado resultados impresionantes en varios puntos de referencia, demostrando un mejor rendimiento en AlpacaEval en comparaci贸n con los m茅todos tradicionales. Para m谩s detalles, puedes consultar la [documentaci贸n de ORPO](./orpo.md).

## Notebook de Ejercicios

| T铆tulo           | Descripci贸n | Ejercicio | Enlace | Colab |
|------------------|-------------|-----------|--------|-------|
| Entrenamiento DPO | Aprende c贸mo entrenar modelos usando Optimizaci贸n Directa de Preferencias |  Entrenar un modelo usando el conjunto de datos Anthropic HH-RLHF<br> Usar tu propio conjunto de datos de preferencias<br> Experimentar con diferentes conjuntos de datos de preferencias y tama帽os de modelo | [Notebook](./notebooks/dpo_finetuning_example.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/2_preference_alignment/notebooks/dpo_finetuning_example.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir en Colab"/></a> |
| Entrenamiento ORPO | Aprende c贸mo entrenar modelos usando Optimizaci贸n de Preferencias por Raz贸n de Probabilidades |  Entrenar un modelo usando datos de instrucciones y preferencias<br> Experimentar con diferentes ponderaciones de p茅rdidas<br> Comparar los resultados de ORPO con DPO | [Notebook](./notebooks/orpo_finetuning_example.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/2_preference_alignment/notebooks/orpo_finetuning_example.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir en Colab"/></a> |

## Recursos

- [Documentaci贸n de TRL](https://huggingface.co/docs/trl/index) - Documentaci贸n de la biblioteca Transformers Reinforcement Learning (TRL), que implementa varias t茅cnicas de alineaci贸n, incluyendo DPO.
- [Papel de DPO](https://arxiv.org/abs/2305.18290) - Art铆culo de investigaci贸n original que introduce la Optimizaci贸n Directa de Preferencias como una alternativa m谩s simple al RLHF, optimizando directamente los modelos de lenguaje utilizando datos de preferencias.
- [Papel de ORPO](https://arxiv.org/abs/2402.01714) - Introduce la Optimizaci贸n de Preferencias por Raz贸n de Probabilidades, un enfoque novedoso que combina el ajuste de instrucciones y la alineaci贸n de preferencias en una sola etapa de entrenamiento.
- [Gu铆a de RLHF de Argilla](https://argilla.io/blog/mantisnlp-rlhf-part-8/) - Una gu铆a que explica diferentes t茅cnicas de alineaci贸n, incluyendo RLHF, DPO y sus implementaciones pr谩cticas.
- [Blog sobre DPO](https://huggingface.co/blog/dpo-trl) - Gu铆a pr谩ctica sobre c贸mo implementar DPO utilizando la biblioteca TRL con ejemplos de c贸digo y mejores pr谩cticas.
- [Script de ejemplo de TRL sobre DPO](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py) - Script de ejemplo completo que muestra c贸mo implementar el entrenamiento DPO utilizando la biblioteca TRL.
- [Script de ejemplo de TRL sobre ORPO](https://github.com/huggingface/trl/blob/main/examples/scripts/orpo.py) - Implementaci贸n de referencia del entrenamiento ORPO utilizando la biblioteca TRL con opciones detalladas de configuraci贸n.
- [Manual de Alineaci贸n de Hugging Face](https://github.com/huggingface/alignment-handbook) - Gu铆as de recursos y base de c贸digo para alinear modelos de lenguaje utilizando diversas t茅cnicas, incluyendo SFT, DPO y RLHF.