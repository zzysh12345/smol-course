{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exploring Chat Templates with SmolLM2 and Llama 3.2\n",
                "\n",
                "This notebook demonstrates how to use chat templates with the `SmolLM2` and `Llama 3.2` models. Chat templates help structure interactions between users and AI models, ensuring consistent and contextually appropriate responses."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from trl import setup_chat_format\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## SmolLM2 Chat Template\n",
                "\n",
                "Let's explore how to use a chat template with the `SmolLM2` model. We'll define a simple conversation and apply the chat template."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(\"mps\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
                "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define messages for SmolLM2\n",
                "messages = [\n",
                "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
                "    {\n",
                "        \"role\": \"assistant\",\n",
                "        \"content\": \"I'm doing well, thank you! How can I assist you today?\",\n",
                "    },\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Apply chat template without tokenization\n",
                "\n",
                "The tokenizer represents the conversation as a string with special tokens to describe the role of the user and the assistant.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Conversation with template: <|im_start|>user\n",
                        "Hello, how are you?<|im_end|>\n",
                        "<|im_start|>assistant\n",
                        "I'm doing well, thank you! How can I assist you today?<|im_end|>\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
                "\n",
                "print(\"Conversation with template:\", input_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Decode the conversatio\n",
                "\n",
                "Note that the conversation is represented as above but with a further assistant message.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Conversation decoded: <|im_start|>user\n",
                        "Hello, how are you?<|im_end|>\n",
                        "<|im_start|>assistant\n",
                        "I'm doing well, thank you! How can I assist you today?<|im_end|>\n",
                        "<|im_start|>assistant\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "print(\"Conversation decoded:\", tokenizer.decode(token_ids=input_text))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tokenize the conversation\n",
                "\n",
                "Of course, the tokenizer also tokenizes the conversation and special token as ids that relate to the model's vocabulary.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Conversation tokenized: [1, 4093, 198, 19556, 28, 638, 359, 346, 47, 2, 198, 1, 520, 9531, 198, 57, 5248, 2567, 876, 28, 9984, 346, 17, 1073, 416, 339, 4237, 346, 1834, 47, 2, 198, 1, 520, 9531, 198]\n"
                    ]
                }
            ],
            "source": [
                "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
                "\n",
                "print(\"Conversation tokenized:\", input_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "This notebook demonstrated how to apply chat templates to different models, `SmolLM2` and `Llama 3.2`. By structuring interactions with chat templates, we can ensure that AI models provide consistent and contextually relevant responses."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
