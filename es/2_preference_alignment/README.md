# Alineaci贸n de Preferencias

Este m贸dulo cubre t茅cnicas para alinear modelos de lenguaje con las preferencias humanas. Mientras que la afinaci贸n supervisada (SFT) ayuda a los modelos a aprender tareas, la alineaci贸n de preferencias fomenta que las salidas coincidan con las expectativas y valores humanos.

## Descripci贸n General

Los m茅todos t铆picos de alineaci贸n incluyen m煤ltiples etapas:
1. Afinaci贸n Supervisada (SFT) para adaptar los modelos a dominios espec铆ficos.
2. Alineaci贸n de preferencias (como RLHF o DPO) para mejorar la calidad de las respuestas.

Enfoques alternativos como ORPO combinan la afinaci贸n por instrucciones y la alineaci贸n de preferencias en un solo proceso. Aqu铆, nos enfocaremos en los algoritmos DPO y ORPO.

Si deseas aprender m谩s sobre las diferentes t茅cnicas de alineaci贸n, puedes leer m谩s sobre ellas en el [Blog de Argilla](https://argilla.io/blog/mantisnlp-rlhf-part-8).

### 1锔 Optimizaci贸n Directa de Preferencias (DPO)

La Optimizaci贸n Directa de Preferencias (DPO) simplifica la alineaci贸n de preferencias optimizando directamente los modelos utilizando datos de preferencias. Este enfoque elimina la necesidad de modelos de recompensa separados y de un aprendizaje por refuerzo complejo, lo que lo hace m谩s estable y eficiente que el Aprendizaje por Refuerzo de Retroalimentaci贸n Humana (RLHF) tradicional. Para m谩s detalles, puedes consultar la [documentaci贸n de Optimizaci贸n Directa de Preferencias (DPO)](./dpo.md).

### 2锔 Optimizaci贸n de Preferencias por Ratio de Probabilidades (ORPO)

ORPO introduce un enfoque combinado para la afinaci贸n por instrucciones y la alineaci贸n de preferencias en un solo proceso. Modifica el objetivo est谩ndar del modelado de lenguaje combinando la p茅rdida de verosimilitud logar铆tmica negativa con un t茅rmino de ratio de probabilidades a nivel de token. El enfoque presenta un proceso de entrenamiento de una sola etapa, una arquitectura libre de modelo de referencia y una mayor eficiencia computacional. ORPO ha mostrado resultados impresionantes en varios puntos de referencia, demostrando un mejor rendimiento en AlpacaEval en comparaci贸n con los m茅todos tradicionales. Para m谩s detalles, puedes consultar la [documentaci贸n de Optimizaci贸n de Preferencias por Ratio de Probabilidades (ORPO)](./orpo.md).

## Notebooks de Ejercicios

| Titulo | Descripci贸n | Ejercicio | Enlace | Colab |
|-------|-------------|----------|------|-------|
| Entrenamiento DPO | Aprende a entrenar modelos utilizando la Optimizaci贸n Directa de Preferencias |  Entrenar un modelo utilizando el conjunto de datos HH-RLHF de Anthropic<br> Utiliza tu propio conjunto de datos de preferencias<br> Experimenta con diferentes conjuntos de datos de preferencias y tama帽os de modelo | [Notebook](./notebooks/dpo_finetuning_example.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/2_preference_alignment/notebooks/dpo_finetuning_example.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir en Colab"/></a> |
| Entrenamiento ORPO | Aprende a entrenar modelos utilizando la Optimizaci贸n de Preferencias por Ratio de Probabilidades |  Entrenar un modelo utilizando datos de instrucciones y preferencias<br> Experimenta con diferentes ponderaciones de la p茅rdida<br> Compara los resultados de ORPO con DPO | [Notebook](./notebooks/orpo_finetuning_example.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/2_preference_alignment/notebooks/orpo_finetuning_example.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir en Colab"/></a> |

## Recursos

- [Documentaci贸n de TRL](https://huggingface.co/docs/trl/index) - Documentaci贸n para la librer铆a Transformers Reinforcement Learning (TRL), que implementa diversas t茅cnicas de alineaci贸n, incluyendo DPO.
- [Papel de DPO](https://arxiv.org/abs/2305.18290) - Art铆culo original de investigaci贸n que introduce la Optimizaci贸n Directa de Preferencias como una alternativa m谩s simple al RLHF que optimiza directamente los modelos de lenguaje utilizando datos de preferencias.
- [Papel de ORPO](https://arxiv.org/abs/2403.07691) - Introduce la Optimizaci贸n de Preferencias por Ratio de Probabilidades, un enfoque novedoso que combina la afinaci贸n por instrucciones y la alineaci贸n de preferencias en una sola etapa de entrenamiento.
- [Gu铆a de RLHF de Argilla](https://argilla.io/blog/mantisnlp-rlhf-part-8/) - Una gu铆a que explica diferentes t茅cnicas de alineaci贸n, incluyendo RLHF, DPO y sus implementaciones pr谩cticas.
- [Entrada en el Blog sobre DPO](https://huggingface.co/blog/dpo-trl) - Gu铆a pr谩ctica sobre c贸mo implementar DPO utilizando la librer铆a TRL con ejemplos de c贸digo y mejores pr谩cticas.
- [Script de ejemplo de TRL sobre DPO](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py) - Script completo de ejemplo que demuestra c贸mo implementar el entrenamiento DPO utilizando la librer铆a TRL.
- [Script de ejemplo de TRL sobre ORPO](https://github.com/huggingface/trl/blob/main/examples/scripts/orpo.py) - Implementaci贸n de referencia del entrenamiento ORPO utilizando la librer铆a TRL con opciones detalladas de configuraci贸n.
- [Manual de Alineaci贸n de Hugging Face](https://github.com/huggingface/alignment-handbook) - Gu铆as y c贸digo para alinear modelos de lenguaje utilizando diversas t茅cnicas, incluyendo SFT, DPO y RLHF.
